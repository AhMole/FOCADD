---
title: "analysis"
author: "Lennart Roesemeier"
date: "May 28, 2020"
output: html_document
---



```{r}
library(NLP)
library(tm)
library(quanteda)
library(tidyverse)
library(stringi)
library(utf8)
library(tidytext)
library(data.table)
library(RColorBrewer)
webshot::install_phantomjs()
library(webshot)
library(rtweet)
library(htmlwidgets)
library(wordcloud)
library(wordcloud2)
library(saotd)
library(RCurl)
library(reshape2)

#install.packages("reshape2")
```


```{r}
id <- "1Fy2sfSzqGATNu_lICcUds6r9pz-5HL-E"
tweets <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", id))

#tweets <- read.csv("C://Users//Lennart//Documents//Studium//master_sociology//data_computingdisp//term paper//data//dataappend.csv", header=T)

tweets$text <- stri_encode(tweets$text, "", "UTF-8")

tweetdata <- data.frame(doc_id = tweets$screen_name, text = tweets$text)
corpus <- Corpus(DataframeSource(tweetdata))

#function to remov URLs
removeURL <- content_transformer(function(x) gsub("(f|ht)tp(s?)://\\S+", "", x, perl=T))


corpus <- tm_map(corpus, removeURL)
d <- data.frame(text = sapply(corpus, as.character), stringsAsFactors = F)

tweets$text <- d$text
```

```{r}
tweets <- as_tibble(tweets)
class(tweets$text)

tweets$text <- tolower(tweets$text)

tweets %>%
  mutate_if(is.factor, as.character) 

tweets$text <- str_replace_all(tweets$text,
                c("[\r\n]" = " ", "[[:punct:]]" = " " , "[^[:alnum:] ]" = " ", "[^a-zA-Z0-9]" = " ", "[\\s[:digit:]]" = " ", "don" = "dont"))


#utf-8 encoding:
#"Ã¢â€šÂ¬" = "â‚¬", "Ã¢â‚¬Å¡" = ",", "Ã¢â‚¬Å¾"= "â€ž", "Ã¢â‚¬Â¦"= "...", "Ã‹â€ "= "Ë†", "Ã¢â‚¬â€œ"= "â€“",  "Ã¢â‚¬â€"= "--", 
#              "Ã¢â‚¬Âº"= "â€º", "Ã‚Â£"= "Â£", "Ã‚Â¥"= "Â¥", "Ã‚Â¦"= "Â¦", "Ã‚Â§"= "Â§", "Ã‚Â¨"= "Â¨", "Ã‚Â«"= "Â«", "Ã‚Â°"= "Â°", 
#              "Ã‚Â²"= "Â²", "Ã‚Â³"= "Â³", "Ã‚Â´"= "Â´", "Ã‚Â¸"= "Â¸", "Ã‚Â»"= "Â»", "Ã‚Â¿"= "Â¿", "ÃƒÅ¸"= "ÃŸ", "ÃƒÂ¡"= "Ã¡", 
#              "ÃƒÂ¢"= "Ã¢", "ÃƒÂ£"= "Ã£", "ÃƒÂ¤"= "Ã¤", "ÃƒÂ¥"= "Ã¥", "ÃƒÂ¨"= "Ã¨", "ÃƒÂ©"= "Ã©", "ÃƒÂª"= "Ãª", "ÃƒÂ«"= "Ã«", 
#              "ÃƒÂ¬"= "Ã¬", "Ãƒ"= "Ã?", "ÃƒÂ®"= "Ã®", "ÃƒÂ¯"= "Ã¯", "ÃƒÂ±"= "Ã±", "ÃƒÂ²"= "Ã²", "ÃƒÂ³"= "Ã³", "ÃƒÂ´"= "Ã´", 
#              "ÃƒÂµ"= "Ãµ", "ÃƒÂ¶"= "Ã¶", "ÃƒÂ¹"= "Ã¹", "ÃƒÂº"= "Ãº", "ÃƒÂ»"= "Ã»", "ÃƒÂ¼"= "Ã¼", "ÃƒÂ½"= "Ã½", "ÃƒÂ¿"= "Ã¿", 
#              "Ãƒ"= "Ã ", "Ã‚"= " ", "Ã¢â‚¬"= "â€", "â€â„¢" = "'", 

setDT(tweets)
cols_rectified <- names(tweets)[vapply(tweets, is.character, logical(1))]
tweets[,c(cols_rectified) := lapply(.SD, trimws), .SDcols = cols_rectified]
tweets[,c(cols_rectified) := lapply(.SD, function(x)gsub("\\s+", " ", x)), .SDcols = cols_rectified]

empty_char <- tweets %>%
  filter(!str_detect(text, ""))

tweets <- tweets %>%
  anti_join(empty_char, by = "text")

rm(empty_char)

```


```{r}
data("stop_words")

custom_stop_words <- bind_rows(tibble(word = c("de", "na", "pic", "sa", "en", "la", "mga", "ng", "el", "di", "ko", "se", "ka", "ve", "si", "pa", "ll", "ce", "hksos", "lahat", "niyo", "sr", "rus", "pra", "han", "hay", "hoy", "ga", "nah", "pc", "ug", "nya", "cuz", "ta", "ad", "ada", "ini", "xl", "es", "da", "isn", "ni", "le", "al", "lo", "ha", "ya", "ma", "rt", "ba", "fu", "eh", "ur", "aren", "hey", "ncov", "dont", "bc", "dr", "las", "st", "hahaha", "lmao", "mas", "il", "em", "yo", "jo", "ne", "jg", "wwg", "tu", "ny", "des", "deste", "jan", "kay", "ain", "omg", "je", "btw", "idk", "pm", "sb", "ww", "au", "fr", "pla", "aoc", "ca", "nyo", "qu", "ses", "asap", "li", "te", "wala", "kasi", "dude", "ser", "set", "wtf", "ah", "ano", "um", "aka", "bts", "cls", "ja", "lam", "su", "abc", "dito", "vp", "hahahahaha", "haha", "nna", "din", "ke", "po", "ppe", "sir", "doesn", "lol", "xi"), 
                                      lexicon = c("custom")), 
                               stop_words)

tweetwords <- tweets %>%
  unnest_tokens(word, text) %>%
  anti_join(custom_stop_words) %>%
  count(word,  sort=T) %>% #56851
  filter(n > 10) #4467 #more or less clean dataset w/ words which occur more than 10 times

# first inspection of top words
tweetwords %>%
  filter(n > 1000) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()
```


```{r fig.height = 11, fig.width = 9}
#topwords get displayed in a wordcloud, wordcloud is a .html file on github repo
wcloud <- tweetwords %>%
  anti_join(stop_words) %>%
  filter(n > 100) %>%
  mutate(word = reorder(word, n))

cloud <- wordcloud2(wcloud, size = 3)

saveWidget(cloud, "wcloud.html", selfcontained = F)
```

```{r}

sentiment <- tweetwords %>%
  rename("amount" = "n") %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, amount, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative) %>%
  arrange(desc(amount))

dplyr::count(sentiment, negative, positive)

```

```{r}
sentiment %>%
  filter(amount > 50) %>%
  ggplot(aes(word, sentiment)) +
  geom_col() +
  theme(axis.text = element_blank())
```



```{r}
sentiment %>%
  count(word, sentiment, amount) %>%
  acast(word ~ sentiment, value.var = "amount", fill = 0) %>%
  magrittr::set_colnames(c("negative", "positive")) %>%
  comparison.cloud(colors = c("red", "blue"),
                   max.words = 100, random.order = F, 
                   match.colors = T)
```






