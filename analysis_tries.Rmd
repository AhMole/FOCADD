---
title: "analysis"
author: "Lennart Roesemeier"
date: "May 28, 2020"
output: html_document
---



```{r, warning=FALSE}
library(NLP)
library(tm)
library(quanteda)
library(tidyverse)
library(stringi)
library(utf8)
library(tidytext)
library(data.table)
library(RColorBrewer)
webshot::install_phantomjs()
library(webshot)
library(rtweet)
library(htmlwidgets)
library(wordcloud)
library(wordcloud2)
library(saotd)
library(RCurl)
library(reshape2)
library(lubridate)

#install.packages("lubridate")
```


```{r warning=FALSE}
id <- "1Fy2sfSzqGATNu_lICcUds6r9pz-5HL-E"
tweets <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", id))

#tweets <- read.csv("C://Users//Lennart//Documents//Studium//master_sociology//data_computingdisp//term paper//data//dataappend.csv", header=T)

tweets$text <- stri_encode(tweets$text, "", "UTF-8")

tweetdata <- data.frame(doc_id = tweets$screen_name, text = tweets$text)
corpus <- Corpus(DataframeSource(tweetdata))

#function to remov URLs
removeURL <- content_transformer(function(x) gsub("(f|ht)tp(s?)://\\S+", "", x, perl=T))


corpus <- tm_map(corpus, removeURL)
d <- data.frame(text = sapply(corpus, as.character), stringsAsFactors = F)

tweets$text <- d$text
```

```{r warning=FALSE}
tweets <- as_tibble(tweets)
class(tweets$text)

tweets$text <- tolower(tweets$text)

tweets %>%
  mutate_if(is.factor, as.character) 

tweets$text <- str_replace_all(tweets$text,
                c("[\r\n]" = " ", "[[:punct:]]" = " " , "[^[:alnum:] ]" = " ", "[^a-zA-Z0-9]" = " ", "[\\s[:digit:]]" = " ", "don" = "dont"))

setDT(tweets)
cols_rectified <- names(tweets)[vapply(tweets, is.character, logical(1))]
tweets[,c(cols_rectified) := lapply(.SD, trimws), .SDcols = cols_rectified]
tweets[,c(cols_rectified) := lapply(.SD, function(x)gsub("\\s+", " ", x)), .SDcols = cols_rectified]

empty_char <- tweets %>%
  filter(!str_detect(text, ""))

tweets <- tweets %>%
  anti_join(empty_char, by = "text")

rm(empty_char)
rm(d)
```


```{r warning=FALSE}
data("stop_words")

custom_stop_words <- bind_rows(tibble(word = c("de", "na", "pic", "sa", "en", "la", "mga", "ng", "el", "di", "ko", "se", "ka", "ve", "si", "pa", "ll", "ce", "hksos", "lahat", "niyo", "sr", "rus", "pra", "han", "hay", "hoy", "ga", "nah", "pc", "ug", "nya", "cuz", "ta", "ad", "ada", "ini", "xl", "es", "da", "isn", "ni", "le", "al", "lo", "ha", "ya", "ma", "rt", "ba", "fu", "eh", "ur", "aren", "hey", "ncov", "dont", "bc", "dr", "las", "st", "hahaha", "lmao", "mas", "il", "em", "yo", "jo", "ne", "jg", "wwg", "tu", "ny", "des", "deste", "jan", "kay", "ain", "omg", "je", "btw", "idk", "pm", "sb", "ww", "au", "fr", "pla", "aoc", "ca", "nyo", "qu", "ses", "asap", "li", "te", "wala", "kasi", "dude", "ser", "set", "wtf", "ah", "ano", "um", "aka", "bts", "cls", "ja", "lam", "su", "abc", "dito", "vp", "hahahahaha", "haha", "nna", "din", "ke", "po", "ppe", "sir", "doesn", "lol", "xi", "ang", "ako", "mo", "lang", "nga", "kung", "naman", "kaya", "kayo", "pero", "yung", "yan", "twitter"), 
                                      lexicon = c("custom")), 
                               stop_words)

tweetwords <- tweets %>%
  unnest_tokens(word, text) %>%
  anti_join(custom_stop_words) %>%
  count(word,  sort=T) %>% #56851
  filter(n > 10) #4467 #more or less clean dataset w/ words which occur more than 10 times

# first inspection of top words
tweetwords %>%
  filter(n > 1000) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
  labs(title="Topwords of the Tweets", subtitle="Words which occur more than 1000 times", x="Words", y="Wordcount")
```


```{r fig.height = 11, fig.width = 9, warning=FALSE}
#topwords get displayed in a wordcloud, wordcloud is a .html file on github repo
wcloud <- tweetwords %>%
  anti_join(stop_words) %>%
  filter(n > 100) %>%
  mutate(word = reorder(word, n))

cloud <- wordcloud2(wcloud, size = 3)
saveWidget(cloud, "wcloud.html", selfcontained = F)
webshot("wcloud.html", "wcloud.png", delay = 5, vwidth = 2000, vheight = 2000)
```

```{r warning=FALSE}
sentiment <- tweetwords %>%
  rename("amount" = "n") %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, amount, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative) %>%
  arrange(desc(amount))

dplyr::count(sentiment, negative, positive)

```

```{r fig.width=14, fig.height=5, warning=FALSE}
label <- c("negative", "l", "0.0", "m", "positive")
sentiment %>%
  filter(amount > 50) %>%
  ggplot(aes(word, sentiment)) +
  geom_col() +
  labs(title="Positive/Negative Sentiments of the Tweets", y="Negative        Sentiment        Positive", x="Contribution to Sentiment") +
  theme(axis.title.x = element_text(),
        axis.text.x = element_text(color = "black", angle = 90),
        axis.ticks.x = element_blank(),
        axis.title.y = element_text(),
        axis.text.y = element_text(color = "black"),
        axis.ticks.y = element_line()) 
```



```{r warning=FALSE}
sentiment %>%
  count(word, sentiment, amount) %>%
  acast(word ~ sentiment, value.var = "amount", fill = 0) %>%
  magrittr::set_colnames(c("negative", "positive")) %>%
  comparison.cloud(colors = c("red", "blue"),
                   max.words = 100, random.order = F, 
                   match.colors = T)
```



```{r warning=FALSE}
tweetsweek1 <- tweets %>%
  select(timestamp, text) %>%
  rename("date" = "timestamp", "tweet" = "text") %>%
  mutate(day=day(strptime(date, "%Y-%m-%d %H:%M:%S")) %>% as.character()) %>%
  mutate(month=month(strptime(date, "%Y-%m-%d %H:%M:%S")) %>% as.character()) %>%
  mutate(year=year(strptime(date, "%Y-%m-%d %H:%M:%S")) %>% as.character()) %>%
  arrange(year, month, day) %>%
  unite(newdate, day, month, year, sep = "-", remove = T) %>%
  select(-date) %>%
  rename("date" = "newdate") %>%
  mutate(date = as.Date(date, "%d-%m-%Y")) %>%
  filter(date >= "2020-1-23" & date <= "2020-1-30")


tweetsweek2 <- tweets %>%
  select(timestamp, text) %>%
  rename("date" = "timestamp", "tweet" = "text") %>%
  mutate(day=day(strptime(date, "%Y-%m-%d %H:%M:%S")) %>% as.character()) %>%
  mutate(month=month(strptime(date, "%Y-%m-%d %H:%M:%S")) %>% as.character()) %>%
  mutate(year=year(strptime(date, "%Y-%m-%d %H:%M:%S")) %>% as.character()) %>%
  arrange(year, month, day) %>%
  unite(newdate, day, month, year, sep = "-", remove = T) %>%
  select(-date) %>%
  rename("date" = "newdate") %>%
  mutate(date = as.Date(date, "%d-%m-%Y")) %>%
  filter(date >= "2020-4-23" & date <= "2020-4-30")
```

```{r warning=FALSE}
week1words <- tweetsweek1 %>%
  unnest_tokens(word, tweet) %>%
  anti_join(custom_stop_words) %>%
  count(word,  sort=T) %>%
  top_n(20)

week2words <- tweetsweek2 %>%
  unnest_tokens(word, tweet) %>%
  anti_join(custom_stop_words) %>%
  count(word,  sort=T) %>%
  top_n(20)
```


```{r warning=FALSE}
week1words %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
  labs(title = "Topwords Occurance", subtitle = "23 Jan 2020 - 30 Jan 2020", y = "Occurance Count")
```

```{r warning=FALSE}
week2words %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +labs(title = "Topwords Occurance", subtitle = "23 April 2020 - 30 April 2020", y = "Occurance Count")
```

```{r}
weekwords <- week1words %>%
  bind_rows(week2words) %>%
  mutate(week = case_when(n == 619 | n == 295 | n == 211 | n == 199 | n == 173 | n == 166 | n == 159 | n == 153 | n == 103 | n == 91 | n == 59 | n == 48 |
                          n == 46 | n == 40 | n == 38 | n == 37 | n == 36 | n == 32 | n == 31 | n == 30 ~ "Week Jan",
                          n == 1590 | n == 1005 | n == 974 | n == 590 | n == 495 | n == 433 | n == 309 | n == 278 | n == 272 | n == 212 | n == 202 | n == 189 |
                          n == 187 | n == 169 | n == 165 | n == 156 | n == 151 | n == 141 | n == 126 | n == 116 ~ "Week April"))
  

weekwords %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = week)) +
  geom_col(show.legend = F) +
  facet_wrap(~week, scales = "free_y") +
  labs(title = "Topwords Occurance",
       subtitle = "23 Jan - 30 Jan 2020 and 23 April - 30 April 2020",
       y=NULL, 
       x=NULL) +
  coord_flip()
```



```{r wordcloud, warning=FALSE}
wc_week1 <- week1words %>%
  mutate(word = reorder(word, n)) %>%
  mutate_if(is.factor, as.character)

cloud1 <- wordcloud2(wc_week1)
saveWidget(cloud1, "wcloud_week1.html", selfcontained = F)
webshot("wcloud_week1.html", "wc_week1.png", delay = 5, vwidth = 2000, vheight = 2000)

wc_week2 <- week2words %>%
  mutate(word = reorder(word, n)) %>%
  mutate_if(is.factor, as.character)

cloud2 <- wordcloud2(wc_week2)
saveWidget(cloud2, "wcloud_week2.html", selfcontained = F)
webshot("wcloud_week2.html", "wc_week2.png", delay = 5, vwidth = 2000, vheight = 2000)
```

![wordcloud](wc_week1.png)
![wordcloud](wc_week2.png)
