---
title: "analysis"
author: "Lennart Roesemeier"
date: "May 28, 2020"
output: html_document
---



```{r}
library(NLP)
library(tm)
library(quanteda)
library(tidyverse)
library(stringi)
library(utf8)
library(tidytext)
library(data.table)
library(RColorBrewer)
webshot::install_phantomjs()
library(webshot)
library(rtweet)
library(htmlwidgets)
library(wordcloud)
library(wordcloud2)
library(saotd)
library(RCurl)
library(reshape2)
library(lubridate)

#install.packages("lubridate")
```


```{r}
id <- "1Fy2sfSzqGATNu_lICcUds6r9pz-5HL-E"
tweets <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download", id))

#tweets <- read.csv("C://Users//Lennart//Documents//Studium//master_sociology//data_computingdisp//term paper//data//dataappend.csv", header=T)

tweets$text <- stri_encode(tweets$text, "", "UTF-8")

tweetdata <- data.frame(doc_id = tweets$screen_name, text = tweets$text)
corpus <- Corpus(DataframeSource(tweetdata))

#function to remov URLs
removeURL <- content_transformer(function(x) gsub("(f|ht)tp(s?)://\\S+", "", x, perl=T))


corpus <- tm_map(corpus, removeURL)
d <- data.frame(text = sapply(corpus, as.character), stringsAsFactors = F)

tweets$text <- d$text
```

```{r}
tweets <- as_tibble(tweets)
class(tweets$text)

tweets$text <- tolower(tweets$text)

tweets %>%
  mutate_if(is.factor, as.character) 

tweets$text <- str_replace_all(tweets$text,
                c("[\r\n]" = " ", "[[:punct:]]" = " " , "[^[:alnum:] ]" = " ", "[^a-zA-Z0-9]" = " ", "[\\s[:digit:]]" = " ", "don" = "dont"))

setDT(tweets)
cols_rectified <- names(tweets)[vapply(tweets, is.character, logical(1))]
tweets[,c(cols_rectified) := lapply(.SD, trimws), .SDcols = cols_rectified]
tweets[,c(cols_rectified) := lapply(.SD, function(x)gsub("\\s+", " ", x)), .SDcols = cols_rectified]

empty_char <- tweets %>%
  filter(!str_detect(text, ""))

tweets <- tweets %>%
  anti_join(empty_char, by = "text")

rm(empty_char)

```


```{r}
data("stop_words")

custom_stop_words <- bind_rows(tibble(word = c("de", "na", "pic", "sa", "en", "la", "mga", "ng", "el", "di", "ko", "se", "ka", "ve", "si", "pa", "ll", "ce", "hksos", "lahat", "niyo", "sr", "rus", "pra", "han", "hay", "hoy", "ga", "nah", "pc", "ug", "nya", "cuz", "ta", "ad", "ada", "ini", "xl", "es", "da", "isn", "ni", "le", "al", "lo", "ha", "ya", "ma", "rt", "ba", "fu", "eh", "ur", "aren", "hey", "ncov", "dont", "bc", "dr", "las", "st", "hahaha", "lmao", "mas", "il", "em", "yo", "jo", "ne", "jg", "wwg", "tu", "ny", "des", "deste", "jan", "kay", "ain", "omg", "je", "btw", "idk", "pm", "sb", "ww", "au", "fr", "pla", "aoc", "ca", "nyo", "qu", "ses", "asap", "li", "te", "wala", "kasi", "dude", "ser", "set", "wtf", "ah", "ano", "um", "aka", "bts", "cls", "ja", "lam", "su", "abc", "dito", "vp", "hahahahaha", "haha", "nna", "din", "ke", "po", "ppe", "sir", "doesn", "lol", "xi", "ang", "ako", "mo", "lang", "nga", "kung", "naman", "kaya", "kayo", "pero", "yung", "yan", "twitter"), 
                                      lexicon = c("custom")), 
                               stop_words)

tweetwords <- tweets %>%
  unnest_tokens(word, text) %>%
  anti_join(custom_stop_words) %>%
  count(word,  sort=T) %>% #56851
  filter(n > 10) #4467 #more or less clean dataset w/ words which occur more than 10 times

# first inspection of top words
tweetwords %>%
  filter(n > 1000) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()
```


```{r fig.height = 11, fig.width = 9}
#topwords get displayed in a wordcloud, wordcloud is a .html file on github repo
wcloud <- tweetwords %>%
  anti_join(stop_words) %>%
  filter(n > 100) %>%
  mutate(word = reorder(word, n))

cloud <- wordcloud2(wcloud, size = 3)
saveWidget(cloud, "wcloud.html", selfcontained = F)
webshot("wcloud.html", "wcloud.png", delay = 5, vwidth = 2000, vheight = 2000)
```

```{r}
sentiment <- tweetwords %>%
  rename("amount" = "n") %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, amount, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative) %>%
  arrange(desc(amount))

dplyr::count(sentiment, negative, positive)

```

```{r}
sentiment %>%
  filter(amount > 50) %>%
  ggplot(aes(word, sentiment)) +
  geom_col() +
  theme(axis.text = element_blank())
```



```{r}
sentiment %>%
  count(word, sentiment, amount) %>%
  acast(word ~ sentiment, value.var = "amount", fill = 0) %>%
  magrittr::set_colnames(c("negative", "positive")) %>%
  comparison.cloud(colors = c("red", "blue"),
                   max.words = 100, random.order = F, 
                   match.colors = T)
```



```{r}
tweetsweek1 <- tweets %>%
  select(timestamp, text) %>%
  rename("date" = "timestamp", "tweet" = "text") %>%
  mutate(day=day(strptime(date, "%Y-%m-%d %H:%M:%S")) %>% as.character()) %>%
  mutate(month=month(strptime(date, "%Y-%m-%d %H:%M:%S")) %>% as.character()) %>%
  mutate(year=year(strptime(date, "%Y-%m-%d %H:%M:%S")) %>% as.character()) %>%
  arrange(year, month, day) %>%
  unite(newdate, day, month, year, sep = "-", remove = T) %>%
  select(-date) %>%
  rename("date" = "newdate") %>%
  mutate(date = as.Date(date, "%d-%m-%Y")) %>%
  filter(date >= "2020-1-23" & date <= "2020-1-30")


tweetsweek2 <- tweets %>%
  select(timestamp, text) %>%
  rename("date" = "timestamp", "tweet" = "text") %>%
  mutate(day=day(strptime(date, "%Y-%m-%d %H:%M:%S")) %>% as.character()) %>%
  mutate(month=month(strptime(date, "%Y-%m-%d %H:%M:%S")) %>% as.character()) %>%
  mutate(year=year(strptime(date, "%Y-%m-%d %H:%M:%S")) %>% as.character()) %>%
  arrange(year, month, day) %>%
  unite(newdate, day, month, year, sep = "-", remove = T) %>%
  select(-date) %>%
  rename("date" = "newdate") %>%
  mutate(date = as.Date(date, "%d-%m-%Y")) %>%
  filter(date >= "2020-4-23" & date <= "2020-4-30")
```

```{r}
week1words <- tweetsweek1 %>%
  unnest_tokens(word, tweet) %>%
  anti_join(custom_stop_words) %>%
  count(word,  sort=T) %>%
  top_n(20)

week2words <- tweetsweek2 %>%
  unnest_tokens(word, tweet) %>%
  anti_join(custom_stop_words) %>%
  count(word,  sort=T) %>%
  top_n(20)
```


```{r}
week1words %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()
```

```{r}
week2words %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()
```

```{r wordcloud}
wc_week1 <- week1words %>%
  mutate(word = reorder(word, n)) %>%
  mutate_if(is.factor, as.character)

cloud1 <- wordcloud2(wc_week1)
saveWidget(cloud1, "wcloud_week1.html", selfcontained = F)
webshot("wcloud_week1.html", "wc_week1.png", delay = 5, vwidth = 2000, vheight = 2000)

wc_week2 <- week2words %>%
  mutate(word = reorder(word, n)) %>%
  mutate_if(is.factor, as.character)

cloud2 <- wordcloud2(wc_week2)
saveWidget(cloud2, "wcloud_week2.html", selfcontained = F)
webshot("wcloud_week2.html", "wc_week2.png", delay = 5, vwidth = 2000, vheight = 2000)
```

![wordcloud](wc_week1.png)
![wordcloud](wc_week2.png)
